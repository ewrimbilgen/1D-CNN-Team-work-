# -*- coding: utf-8 -*-
"""COVID19 Prediction using CNN1D.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pNeEop8Lhwyc4_wMfcRKkYuP3odpstSK

# CNN 1D Model for Covid-19 Cases and Fatalities in Poland and the EU

COVID19 Case Study - Analysis - CNN 1D Modelling

Our Dataset: Our data is about Covid19 cases and fatalities around the world

What we want to analyse:

Extensive EDA to understand more about the corona disease.

* Focusing on Poland and comparing its results with the EU.
* Modelling to forecast the future values for Poland (CNN 1D)
* We are going to model Conv1D for the first step to show kernel moves in 1 direction
* Then we will add a feature to build another CNN 1D model fousing on the EU. Then we can compare and interpret our models' results, so we can see how our models' outcomes can evolve.
"""

# Commented out IPython magic to ensure Python compatibility.
import math
import numpy as np 
import pandas as pd 
import seaborn as sns
import os
import matplotlib.pyplot as plt
import plotly.express as px
from datetime import datetime
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import Flatten
from keras.layers.convolutional import Conv1D
from keras.layers.convolutional import MaxPooling1D
from sklearn.preprocessing import StandardScaler

# %matplotlib inline

data = pd.read_csv("data.csv")

"""First of all, let's take a look on the data structure:"""

data.head()

data.info()

"""Checking out the size of the dataset of data"""

data.shape

display(data.info())

display(data.describe())

"""## **Checking the null values**"""

data.isnull().sum()

print("Number of Country_Region: ", data['Country_Region'].nunique())
print("Dates go from day", max(data['Date']), "to day", min(data['Date']), ", a total of", data['Date'].nunique(), "days")
print("Countries with Province/State informed: ", data.loc[data['Province_State']!='None']['Country_Region'].unique())

"""It's time to see correlaretion between features in the data

# Exploratory Data **Analysis**

Visualizations to understand the correlations of attributes with each other and how they function and alter with different distribution
"""

sns.pairplot(data)

confirmed_total_date = data.groupby(['Date']).agg({'ConfirmedCases':['sum']})
fatalities_total_date = data.groupby(['Date']).agg({'Fatalities':['sum']})
total_date = confirmed_total_date.join(fatalities_total_date)

fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(17,7))
total_date.plot(ax=ax1)
ax1.set_title("Global confirmed cases", size=13)
ax1.set_ylabel("Number of cases", size=13)
ax1.set_xlabel("Date", size=13)
fatalities_total_date.plot(ax=ax2, color='orange')
ax2.set_title("Global deceased cases", size=13)
ax2.set_ylabel("Number of cases", size=13)
ax2.set_xlabel("Date", size=13)

"""**Observations:** The global curve shows a rich fine structure"""

confirmed_total_date_Poland = data[data['Country_Region']=='Poland'].groupby(['Date']).agg({'ConfirmedCases':['sum']})
fatalities_total_date_Poland = data[data['Country_Region']=='Poland'].groupby(['Date']).agg({'Fatalities':['sum']})
total_date_Poland = confirmed_total_date_Poland.join(fatalities_total_date_Poland)

fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(17,7))
total_date_Poland.plot(ax=ax1)
ax1.set_title("Poland confirmed cases", size=13)
ax1.set_ylabel("Number of cases", size=13)
ax1.set_xlabel("Date", size=13)
fatalities_total_date_Poland.plot(ax=ax2, color='orange')
ax2.set_title("Poland deceased cases", size=13)
ax2.set_ylabel("Number of cases", size=13)
ax2.set_xlabel("Date", size=13)

"""**Observations:** the general behavior looks cleaner,once it reaches the maximum of the contagion, decreases with a lower slope

As a fraction of the total population of Poland
"""

pop_Poland = 37950000.

total_date_Poland.ConfirmedCases = total_date_Poland.ConfirmedCases/pop_Poland*100.
total_date_Poland.Fatalities = total_date_Poland.ConfirmedCases/pop_Poland*100.

plt.figure(figsize=(15,10))
plt.subplot(2, 2, 1)
total_date_Poland.ConfirmedCases.plot(ax=plt.gca(), title='Poland')
plt.ylabel("Fraction of population infected")
plt.ylim(0, 0.05)

"""**observation:** interesting to see the evolution of the infections from the first confirmed case

## Preparation
* The CNN will learn and map a sequence of past observations as input to an output observation.
* The sequence of observations must be transformed into multiple examples from which the model can learn.
* The sequence can be divided into multiple patterns called samples.

The `get_sequence_split()` function below implements this behavior and will split a given univariate sequence into multiple samples where each sample has a specified number of time steps and the output is a single time step.
"""

data.head()

poland_df = data[data['Country_Region']=="Poland"]
poland_df.tail()

data_to_use = poland_df['ConfirmedCases'].values
data_to_use_f = poland_df['Fatalities'].values
print('Total number of days in the dataset: {}'.format(len(data_to_use)))

# Normalizing the dataset
scaler = StandardScaler()
scaled_dataset = scaler.fit_transform(data_to_use.reshape(-1, 1))
scaled_dataset_f = scaler.fit_transform(data_to_use_f.reshape(-1, 1))

# The function to split a univariate sequence into samples:
# Two examples:
def get_sequence_split(sequence, n_steps):
    X, y = list(), list()
    for i in range(len(sequence)):
        # Find the end of this pattern
        end_ix = i + n_steps
        # Check if we are beyond the sequence
        if end_ix > len(sequence)-1:
            break
        # Gather input and output parts of the pattern
        seq_x, seq_y = sequence[i:end_ix], sequence[end_ix]
        X.append(seq_x)
        y.append(seq_y)
    return array(X), array(y)

def shift_data(data, shift_size):
    X = []
    y = []
    
    i = 0
    while (i + shift_size) <= len(data) - 1:
        X.append(data[i:i+shift_size])
        y.append(data[i+shift_size])
        
        i += 1
    assert len(X) ==  len(y)
    return X, y

X, y = shift_data(scaled_dataset, 7)
X_f, y_f = shift_data(scaled_dataset_f, 7)

X_train  = np.array(X[:60])
y_train = np.array(y[:60])

X_test = np.array(X[60:])
y_test = np.array(y[60:])


X_train_f  = np.array(X_f[:60])
y_train_f = np.array(y_f[:60])

X_test_f = np.array(X_f[60:])
y_test_f = np.array(y_f[60:])

print("X_train size: {}".format(X_train.shape))
print("y_train size: {}".format(y_train.shape))
print("X_test size: {}".format(X_test.shape))
print("y_test size: {}".format(y_test.shape))

dates = poland_df["Date"]
train_date = dates[:60]
test_date = dates[-10:]

# choose a number of time steps
n_steps = 7

"""## **CNN Model**
A one-dimensional CNN is a model that has a convolutional hidden layer that operates over a 1D sequence. This is followed by perhaps a second convolutional layer in some cases, such as very long input sequences, and then a pooling layer whose job it is to distill the output of the convolutional layer to the most salient elements.

The convolutional and pooling layers are followed by a dense fully connected layer that interprets the features extracted by the convolutional part of the model. A flatten layer is used between the convolutional layers and the dense layer to reduce the feature maps to a single one-dimensional vector.
"""

# We can define a 1D CNN Model for univariate time series forecasting as follows.
# Learning for Confirmed Cases
X = X_train
y = y_train

n_features = 1
X = X.reshape((X.shape[0], X.shape[1], n_features))
# define model
model = Sequential()
model.add(Conv1D(filters=32, kernel_size=2, activation='relu', input_shape=(n_steps, n_features)))
model.add(MaxPooling1D(pool_size=2))
model.add(Flatten())
model.add(Dense(50, activation='relu'))
model.add(Dense(1))
model.compile(optimizer='adam', loss='mse')
# fit model
model.fit(X, y, epochs=5000, verbose=0)

"""Key in the definition is the shape of the input; that is what the model expects as input for each sample in terms of the number of time steps and the number of features.

* We are working with a univariate series, so the number of features is one, for one variable. Also, the rectified linear unit is the activation function. 

* We define a convolutional layer with 32 filter maps and a kernel size of 2.

* We define max pooling layer and a dense layer to interpret the input feature.

* The number of time steps as input is the number we chose when preparing our dataset as an argument to the sequence splitting function, which is 7 here.

* The model is fit using the efficient Adam version of stochastic gradient 
descent and optimized using the mean squared error, or `‘mse‘`, loss function.
"""

y_pred=[]
for i in X_test:
    
    x_input = i
    x_input = x_input.reshape((1, n_steps, n_features))
    yhat = model.predict(x_input, verbose=0)
    y_pred.append(float(yhat[0]))

    
print(y_pred)
print(y_test)

plt.figure(figsize = (18,9))
plt.plot(dates,scaled_dataset,label="Data Confirmed Cases")
plt.plot(test_date,y_pred,label="Predicted Confirmed Cases")
plt.plot(test_date,y_test,label="Actual Confirmed Cases")
plt.xticks(rotation=90)
plt.xlabel('Date',fontsize=18)
plt.legend()
plt.title("Confirmed Cases using CNN")
plt.show()

# Learning for Fatalities:
X = X_train_f
y = y_train_f

n_features = 1
X = X.reshape((X.shape[0], X.shape[1], n_features))
print(X.shape)
# define model
model = Sequential()
model.add(Conv1D(filters=32, kernel_size=2, activation='relu', input_shape=(n_steps, n_features)))
model.add(MaxPooling1D(pool_size=2))
model.add(Flatten())
model.add(Dense(50, activation='relu'))
model.add(Dense(1))
model.compile(optimizer='adam', loss='mse')
# fit model
model.fit(X, y, epochs=5000, verbose=0)

# Prediction of Fatalities:
y_pred_f=[]
for i in X_test_f:
    
    x_input = i
    x_input = x_input.reshape((1, n_steps, n_features))
    yhat = model.predict(x_input, verbose=0)
    y_pred_f.append(((float(yhat[0]))))

    
print(y_pred_f)

plt.figure(figsize = (18,9))
plt.plot(dates,scaled_dataset_f,label="Data Fatalities")
plt.plot(test_date,y_pred_f,label="Predicted Fatalities")
plt.plot(test_date,y_test_f,label="Actual Fatalities")
plt.xticks(rotation=90)
plt.xlabel('Date',fontsize=18)
plt.legend()
plt.title("Fatalities Using CNN")
plt.show()

model.summary()

model.evaluate(y, y_pred_f, verbose=0)

model.metrics_names

"""Loss is a scalar value that we attempt to minimize during our training of the model. The lower the loss, the closer our predictions are to the true labels.

# Extending Predictions on EU countries
"""

data["Country_Region"].unique()

!pip install countrygroups
from countrygroups import EUROPEAN_UNION

EUROPEAN_UNION.names

target_countries = EUROPEAN_UNION.names

data["IS_EUROPEAN"] = np.where(data["Country_Region"].isin(target_countries), 1, 0)
countries_matched = data[data["IS_EUROPEAN"] == 1]["Country_Region"].unique()

print(data[data["IS_EUROPEAN"] == 1]["Country_Region"].unique())
print("\nThere are {} european countries and I matched {} countries".format(
    len(target_countries),
    len(countries_matched)
))

eu_train = data[data["IS_EUROPEAN"] == 1]
eu_train = eu_train.groupby(by=["Date"]).sum()
eu_train.tail()

train_confirmed_cases = eu_train['ConfirmedCases'].values
train_fatalities = eu_train['Fatalities'].values
print('Total number of days in the dataset: {}'.format(len(data_to_use)))

# Normalizing the dataset
scaler = StandardScaler()
scaled_dataset = scaler.fit_transform(train_confirmed_cases.reshape(-1, 1))
scaled_dataset_f = scaler.fit_transform(train_fatalities.reshape(-1, 1))

X, y = shift_data(scaled_dataset, 7)
X_f, y_f = shift_data(scaled_dataset_f, 7)

X_train  = np.array(X[:60])
y_train = np.array(y[:60])

X_test = np.array(X[60:])
y_test = np.array(y[60:])


X_train_f  = np.array(X_f[:60])
y_train_f = np.array(y_f[:60])

X_test_f = np.array(X_f[60:])
y_test_f = np.array(y_f[60:])

print("X_train size: {}".format(X_train.shape))
print("y_train size: {}".format(y_train.shape))
print("X_test size: {}".format(X_test.shape))
print("y_test size: {}".format(y_test.shape))

dates = eu_train.index # DATE IS INDEX AFTER GROUP BY
train_date = dates[:60]
test_date = dates[-10:]

# choose a number of time steps
n_steps = 7

X = X_train
y = y_train

n_features = 1
X = X.reshape((X.shape[0], X.shape[1], n_features))
# define model
model = Sequential()
model.add(Conv1D(filters=32, kernel_size=2, activation='relu', input_shape=(n_steps, n_features)))
model.add(MaxPooling1D(pool_size=2))
model.add(Flatten())
model.add(Dense(50, activation='relu'))
model.add(Dense(1))
model.compile(optimizer='adam', loss='mse')
# fit model
model.fit(X, y, epochs=5000, verbose=0)

y_pred=[]
for i in X_test:
    
    x_input = i
    x_input = x_input.reshape((1, n_steps, n_features))
    yhat = model.predict(x_input, verbose=0)
    y_pred.append(float(yhat[0]))

    
print(y_pred)
print(y_test)

plt.figure(figsize = (18,9))
plt.plot(dates,scaled_dataset,label="Data Confirmed Cases")
plt.plot(test_date,y_pred,label="Predicted Confirmed Cases")
plt.plot(test_date,y_test,label="Actual Confirmed Cases")
plt.xticks(rotation=90)
plt.xlabel('Date',fontsize=18)
plt.legend()
plt.title("Confirmed Cases using CNN")
plt.show()

# Learning for Fatalities:
X = X_train_f
y = y_train_f

n_features = 1
X = X.reshape((X.shape[0], X.shape[1], n_features))
print(X.shape)
# define model
model = Sequential()
model.add(Conv1D(filters=32, kernel_size=2, activation='relu', input_shape=(n_steps, n_features)))
model.add(MaxPooling1D(pool_size=2))
model.add(Flatten())
model.add(Dense(50, activation='relu'))
model.add(Dense(1))
model.compile(optimizer='adam', loss='mse')
# fit model
model.fit(X, y, epochs=5000, verbose=0)

# Prediction of Fatalities:
y_pred_f=[]
for i in X_test_f:
    
    x_input = i
    x_input = x_input.reshape((1, n_steps, n_features))
    yhat = model.predict(x_input, verbose=0)
    y_pred_f.append(((float(yhat[0]))))

    
print(y_pred_f)

plt.figure(figsize = (18,9))
plt.plot(dates,scaled_dataset_f,label="Data Fatalities")
plt.plot(test_date,y_pred_f,label="Predicted Fatalities")
plt.plot(test_date,y_test_f,label="Actual Fatalities")
plt.xticks(rotation=90)
plt.xlabel('Date',fontsize=18)
plt.legend()
plt.title("Fatalities Using CNN")
plt.show()

model.summary()

model.evaluate(y, y_pred_f, verbose=0)